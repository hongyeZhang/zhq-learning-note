

## HashMap

* 重点知识点
    - JDK 1.8 中对hash算法和寻址算法是如何优化的
        - hash 算法优化：
            - (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16)
            - 在寻址时能够同时应用高16位和低16位的信息
            - 对每个hash值，在他的低16位中，让高低16位进行了异或，让他的低16位同时保持了高低16位的特征，
              尽量避免一些hash值后续出现冲突，大家可能会进入数组的同一个位置
        - 寻址算法
            - 对数组的长度取模，用与运算替代取模，提升性能
            - hash & (n - 1) -> 效果是跟hash对n取模，效果是一样的，但是与运算的性能要比hash对n取模要高很多，
              数组的长度会一直是2的n次方，只要他保持数组长度是2的n次方
    - 怎样解决 hash 碰撞
        - 假设你的链表很长，可能会导致遍历链表，性能会比较差，O(n)优化，如果链表的长度达到了一定的长度之后，
          其实会把链表转换为红黑树，遍历一颗红黑树找一个元素，此时O(logn)，性能会比链表高一些
    - 如何扩容
        - 判断二进制结果中是否多出一个bit的1，如果没多，那么就是原来的index，如果多了出来，那么就是index + oldCap，
          通过这个方式，就避免了rehash的时候，用每个hash对新数组.length取模，取模性能不高，位运算的性能比较高



### JDK1.7

* 因为在 put 元素的时候，如果触发扩容操作，也就是 rehash ，就会将原数组的内容重新 hash 到新的扩容数组中，但是在
扩容这个过程中，其他线程也在进行 put 操作，如果这两个元素 hash 值相同，可能出现同时在同一数组下用链表表示，造成闭环，
导致在get时会出现死循环，所以HashMap是线程不安全的

* 并发put会造成循环链表，导致get时出现死循环.此问题在JDK8中已经解决。





### JDK1.8
* 存储结构： 数组 + 链表（红黑树，当链表的长度大于8时）
    - 长的链表转化为红黑树的原因： 降低查询的时间复杂度，将O(n)降为O(logN)
* 解决hash碰撞的方法
    - 开放定址法 （ThreadLocal 使用，ThreadLocalMap, 当出现碰撞时，寻找进紧接的下一个地址
    - 再散列法
    - 链地址法 JDK1.8使用
* hash方法：
    - (h = key.hashCode()) ^ (h >>> 16)
    - 拿的是 key 的 hash 值， 异或hash值的高16位（无符号右移16位）
* 计算位置
    - (n-1) & hash
    - 2的整数次幂 - 1 = 1111111 类似的形式，也被称作掩码
    - 只会留下来 n-1 范围内的数据
* put 方法
    - 在第一次放入元素的时候进行初始化，实现延迟初始化
    - 判断当前位置是否有值存在
        - 如果有值
            - 判断当前节点如果是红黑树： 则调用红黑树的相关方法，以后的平衡等
            - 判断当前节点如果是链表： 则调用普通链表的操作方法
            - 判断是否是覆盖旧值，如果可以，则覆盖
        - 如果没有值
            - 直接放入
        - 判断当前的冲突长度
            - 如果大于8，则转化为红黑树
        - 模板方法 给 LinkedHashMap 使用，在当前为空

* resize 扩容
    - 使用一个新的数组代替原来的小数组，尽可能减少hash冲突，提高访问的性能，尽可能保证O(1)
    - 判断当前的容量大小
        - 最大容量 Integer.MAX_VALUE
    - 新数组的大小是原数组大小的 左移 1 位，位移运算符效率高，代替乘法
    - 遍历老数组的元素，移动到新的数组上
        - 如果一个节点后面没有 node 节点， 直接计算在新数组容量下的存储位置
        - 如果一个节点后面有node节点
            - 如果是红黑树，则挪动
            - 如果是链表
                - JDK1.8 扩容优化
                    - 将一个链表拆成两个链表，移动两个链表的头元素
                    - 一个链表是新的桶数组中在 原位置
                    - 另一个链表新的桶数组中在 原位置 + oldCap
                    - 不需要向JDK1.7中再将hash值重新计算一遍
    
* JDK1.8 中 hashmap 存在的问题
    - 1 多线程的put可能导致元素的丢失
    - 2 put和get并发时，可能导致get为null

        
        

* 问题：在什么时候重写 hashcode 和 equals 方法？
    - 在往 hashmap 里面放元素的时候，减少hash冲突，可以自己定义hash函数
      因为 key 的 hashCode 方法会用来计算元素在数组中的位置

## LinkedList

## ConcurrentHashMap
### JDK1.7
* Segment 对象继承了 ReentrantLock 类，因为 Segment 对象它就变成了一把锁，这样就可以保证数据的安全
### JDK1.8

* 实现线程安全的原理
    - 如果你是对数组里同一个位置的元素进行操作，才会加锁串行化处理；如果是对数组不同位置的元素操作，此时大家可以并发执行的
    - 一个大的数组，数组里每个元素进行put操作，都是有一个不同的锁，刚开始进行put的时候，如果两个线程都是在数组[5]这个位置进行put，这个时候，对数组[5]这个位置进行put的时候，采取的是CAS的策略
      同一个时间，只有一个线程能成功执行这个CAS，就是说他刚开始先获取一下数组[5]这个位置的值，null，然后执行CAS，线程1，比较一下，put进去我的这条数据，同时间，其他的线程执行CAS，都会失败
      分段加锁，通过对数组每个元素执行CAS的策略，如果是很多线程对数组里不同的元素执行put，大家是没有关系的，如果其他人失败了，其他人此时会发现说，数组[5]这位置，已经给刚才又人放进去值了


* ，在容器安全上，1.8 中的 ConcurrentHashMap 放弃了 JDK1.7 中的分段技术，而是采用了 CAS 机制 + synchronized 
来保证并发安全性，但是在 ConcurrentHashMap 实现里保留了 Segment 定义，这仅仅是为了保证序列化时的兼容性而已，并没有任何结构上的用处

* CAS 机制虽然无需加锁、安全且高效，但也存在一些缺点，概括如下：
    - 循环检查的时间可能较长，不过可以限制循环检查的次数
    - 只能对一个共享变量执行原子操作
    - 存在 ABA 问题（ABA 问题是指在 CAS 两次检查操作期间，目标变量的值由 A 变为 B，又变回 A，但是 CAS
     看不到这中间的变换，对它来说目标变量的值并没有发生变化，一直是 A，所以 CAS 操作会继续更新目标变量的值。）

*  Key 字段被 final 修饰，说明在生命周期内，key 是不可变的， val 字段被 volatile 修饰了，这就保证了 val 字段的可见性。
  
* put 方法步骤
第一步、在 ConcurrentHashMap 中不允许 key val 字段为空，所以第一步先校验key value 值，key、val 两个字段都不能是 null 才继续往下走，否则直接返回一个 NullPointerException 错误，这点跟 HashMap 有区别，HashMap 是可以允许为空的。
第二步、判断容器是否初始化，如果容器没有初始化，则调用 initTable 方法初始化，initTable 方法如下：
第三步、根据双哈希之后的 hash 值找到数组对应的下标位置，如果该位置未存放节点，也就是说不存在 hash 冲突，则使用 CAS 无锁的方式将数据添加到容器中，并且结束循环。
第四步、如果并未满足第三步，则会判断容器是否正在被其他线程进行扩容操作，如果正在被其他线程扩容，则放弃添加操作，加入到扩容大军中（ConcurrentHashMap 扩容操作采用的是多线程的方式，后面我们会讲到），扩容时并未跳出死循环，这一点就保证了容器在扩容时并不会有其他线程进行数据添加操作，这也保证了容器的安全性。
第五步、如果 hash 冲突，则进行链表操作或者红黑树操作（如果链表树超过8，则修改链表为红黑树），在进行链表或者红黑树操作时，会使用 synchronized 锁把头节点被锁住了，保证了同时只有一个线程修改链表，防止出现链表成环。
第六步、进行 addCount(1L, binCount) 操作，该操作会更新 size 大小，判断是否需要扩容



* initTable() 初始化流程
1、判断 sizeCtl 值是否小于 0，如果小于 0 则表示 ConcurrentHashMap 正在执行初始化操作，所以需要先等待一会，如果其它线程初始化失败还可以顶替上去
2、如果 sizeCtl 值大于等于 0，则基于 CAS 策略抢占标记 sizeCtl 为 -1，表示 ConcurrentHashMap 正在执行初始化，然后构造 table，并更新 sizeCtl 的值



* addCount() 方法
做了两个工作：
1、对 map 的 size 加一
2、检查是否需要扩容，或者是否正在扩容。如果需要扩容，就调用扩容方法，如果正在扩容，就帮助其扩容。


* 有许多的内部类
* 核心部分
    - put 方法
    - helpTransfer 多个线程帮忙挪数据

* TreeNode

* ForwardingNode 
    - 支持扩容操作，将已处理的节点和空节点置为 ForwardingNode 



* transfer（扩容）
    - 第一步：计算出每个线程每次可以处理的个数，根据 Map 的长度，计算出每个线程（CPU）需要处理的桶（table数组的个数），默认每个线程每次处理 16 个桶，如果小于 16
     个，则强制变成 16 个桶。
    - 第二步：对 nextTab 初始化，如果传入的新 table nextTab 为空，则对 nextTab 初始化，默认是原 table 的两倍
    - 第三步：引入 ForwardingNode、advance、finishing 变量来辅助扩容，ForwardingNode 表示该节点已经处理过，不需要在处理，advance
     表示该线程是否可以下移到下一个桶（true：表示可以下移），finishing 表示是否结束扩容（true：结束扩容，false：未结束扩容） ，具体的逻辑就不说了
    - 第四步：跳过一些其他细节，直接到数据迁移这一块，在数据转移的过程中会加 synchronized 锁，锁住头节点，同步化操作，防止 putVal 的时候向链表插入数据
    - 第五步：进行数据迁移，如果这个桶上的节点是链表或者红黑树，则会将节点数据分为低位和高位，计算的规则是通过该节点的 hash 值跟为扩容之前的 table
     容器长度进行位运算（&），如果结果为 0 ，则将数据放在新表的低位（当前 table 中为 第 i 个位置，在新表中还是第 i 个位置），结果不为 0 ，则放在新表的高位（当前 table 中为第 i 个位置，在新表中的位置为 i + 当前 table 容器的长度）。
    - 第六步：如果桶挂载的是红黑树，不仅需要分离出低位节点和高位节点，还需要判断低位和高位节点在新表以链表还是红黑树的形式存放。



* helpTransfer






* put 方法
    - 通过 CAS 实现插入操作
    - 加锁时，锁住桶的头结点
    - 如果节点个数大于8，则进行树化处理

* helpTransfer()
    - 帮助进行扩容
    - 帮助从旧的 table 复制到新的 table中
    - 多个线程并发的条件下，提升 hashmap 扩容的效率的
    - 表的size 小于64时，会触发扩容，不会立即转化为红黑树














## AQS




## Lock 

图片表示

* 同步队列：双向链表
* 条件队列：单向链表



## Condition
* 与 object.wait() signal() 的区别
    - 可以根据条件决定唤起哪一个堵塞的线程，而 notify()不会根据条件判断
    - 


##  ReentrantLock
* 公平锁：侧重公平性
* 非公平锁：抢占式竞争锁，侧重并发性，少了一部判断堵塞队列的过程。
    - 线程不必进入等待队列即可获得锁
    - 减少了CAS的竞争



