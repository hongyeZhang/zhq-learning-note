# 《lucene》学习笔记

## 认识全文检索

* 什么叫做全文检索呢？这要从我们生活中的数据说起。我们生活中的数据总体分为两种：
    * 1. 结构化数据：指具有固定格式或有限长度的数据，如数据库，元数据等。
    * 2. 非结构化数据（也叫全文数据） ：指不定长或无固定格式的数据，如邮件，word文档等。
    * 3. 半结构化数据：如XML，HTML等，当根据需要可按结构化数据来处理，也可抽取出纯文本按非结构化数据来处理。

* 对非结构化数据也即对全文数据的搜索主要有两种方法：
* 1. 顺序扫描法 (Serial Scanning)
所谓顺序扫描，比如要找内容包含某一个字符串的文件，就是一个文档一个文档的看，对于每一个文档，从头看到尾，如果此文档包含此字符串，则此文档为我们要找的文件，
接着看下一个文件，直到扫描完所有的文件。如利用windows的搜索也可以搜索文件内容，只是相当的慢。如果你有一个80G硬盘，如果想在上面找到一个内容包含某字符串的文件，不花他几个小时，怕是做不到。Linux下的grep命令也是这一种方式。
这种方法比较原始，但对于小数据量的文件，这种方法还是最直接，最方便的。但是对于大量的文件，这种方法就很慢了。
* 2. 全文检索
将非结构化数据中的一部分信息提取出来，重新组织，使其变得有一定结构，然后对此有一定结构的数据进行搜索，从而达到搜索相对较快的目的。


### 正排索引
根据文件找关键字

### 倒排索引
* 根据关键字找文件
倒排索引一般表示为一个关键词，然后是它的频度（出现的次数），位置（出现在哪一篇文章或网页中，及有关的日期，作者等信息），它相当于为
互联网上几千亿页网页做了一个索引，好比一本书的目录、标签一般。想看哪一个主题相关的章节，直接根据目录即可找到相关的页面。不必再从书的第一页到最后一页，一页一页的查找

倒排索引2个重要构成：
1. 倒排文件(inverted file)：存储倒排索引的物理文件  
2. 倒排索引组成：单词词典和倒排文件。


## 核心术语
* Index
    * Lucene的索引（Index）是由多个文件组成，这些文件被存放在同一目录下。
* Token
    * 词元（Token）在Lucene与在自然语言处理（NLP）中的概念相同，表示“词元”。词元即自然语言中的基本单位：在中文表现为一个独立的字或词，在英文中表现为一个单词。
* Term
    * 经过分词和语言处理后得到的字符串，它是索引的最小单位。
* Field
    * 不同的域可以指定不同的索引方式，比如指定不同的分词方式，是否构建索引，是否存储等。、
    * 数值型 字符型（StringField 不分词处理） 文本型(TextField 分词处理) 日期型()
* Document
    * 文档是索引的基本单位，代表一些域（Field）的集合。在Lucene中，Document是一种逻辑文件。可以近似地认为它表示的是计算机中的一个文件。
       这个Document是一种抽象的表示，它从各种维度来描述一个数据源中的每一条数据。可以使用关系型数据库中的记录来理解Document，数据库的
       每一条记录可以表示为一个Document，而每一列可以用Field来表示。但不同的是，Document并非结构化的，并没有schema的约束。不同的文档
       保存在不同的段中的，一个段中可以包含多篇文档。新添加的文档被单独保存在一个新生成的段中，随着段的合并，不同的文档会合并到至相同的段中。
* Segment
    * 一个Lucene的索引（Index）由多个段组成，段与段之间是独立的。当添加索引时会生成新的Document，但并不是把新的Document 立即添加到同一个索引文件，
    而是把它们先被写入到不同的小文件（Segment），当小文件的个数达到阈值（段的个数，段中包含的文件数等）时，然后再合并成一个大索引文件（不同的段可以合并）

* Segment_N
    Segments_N为段的元数据信息文件。保存了此索引包含多少个段，每个段包含多少篇文档等信息
    Lucene当前活跃的Segment都会存在一个Segment Info文件里，也就是segments_N。如果有多个segments_N, 那么序号最大的就是最新的 

* segment -> document -> field


## Lucene文档模型

域文件格式
域元数据文件（fnm）
一个段(Segment)包含多个域，每个域都有一些元数据信息，保存在.fnm文件中，.fnm文件的格式如下：
域数据文件（fdt）
真正保存域（stored field）信息的是fdt文件。
假如在一个Segment中包含N篇文档，那么fdt文档中一会有N个项，每一个项都保存对应文档的域信息。
● FieldCount，表示此文档包含的域数目
● 紧接着是FieldCount域信息项，每个项保存一个域的信息。对于每一个域的信息：
	○ FieldNum是域编号
	○ 接着是一个8bit的byte。根据填充值（0或1），代表不同的意义。最低一位表示此域是否分词；倒数第二位表示此域保存的是字符串数据还是二进制数据；倒数第三位表示此域是否被压缩。最后存储的是这个存储域的值。
域索引文件（fdx）
由域数据文件格式可知，每篇文档包含的域的个数、每个存储域的值都是不一样的。
因为segment中会包含N篇文档（Document），每篇文档占用的大小也是不一样的，那么如何快速在fdt文件中辨别每一篇文档的起始地址和终止地址？如何能够更快的找到第N篇文档的存储域的信息呢？
这就需要借助域索引文件。域索引文件也总共有N个项（如果segment中包含N篇Document），每篇文档都对应一个项，每一项中都存储一个Long型数值（大小固定），该数值代表文档在fdt中的起始地址偏移量。

词向量文件
词向量信息是从索引（Index）到文档（Document）到域（Field）到词（Term）的正向信息，有了词向量信息，就可以得到一篇文档包含哪些词的信息 。

词向量数据文件（tvd）
● 首先，是此文档包含域的个数：NumFields
● 之后，是一个NumFields大小的数组，数组每一项都是域号
● 最后，是（NumField - 1）大小的数组。每一篇文档的第一个域的偏移量信息存储在 tvx 文件中。而其他（NumFields - 1）域的偏移量就是第一个域的偏移量加上第（NumField - 1）个数组的值。
词向量索引文件（tvx）
一个段（segment）包含N篇文档，此文件就有N项，每一项代表一篇文档。每一项包含两部分信息：
● 第一部分是词向量文档文件（tvd）中此文档的偏移量
● 第二部分是词向量文件（tvf）中此文档的第一个域的偏移量
词向量域文件（tvf）
该文件包含了此段（Segment）中的所有域，并且不对文档做区分，到底第几个域到第几个域是属于那篇文件，是由tvx文件中的第一个域的偏移量以及tvd文件中的（NumField - 1）个域的偏移量来决定哪些域数据属于哪个文档。
对于每一个域，包含如下项：
● 首先，是说明此域中包含的多少（NumTerms）个词（Term）
● 之后，是8bit的byte（最后一位指定是否保存位置信息，倒数第二位表示是否保存偏移量信息）
● 最后，NumTerms个项的数组，每一项代表一个词（Term）。每一个词，由如下几部分构成
	○ 词的文本TermText
	○ 词频TermFreq（词在该文档中出现的次数）
	○ 词的位置
	○ 词的偏移量







### 创建倒排索引流程

全文检索的索引创建过程一般有以下几步



## 使用
* demo 版本 lucene 7.7.0
* 是否使用复合索引方式  true  .cfs .cfe 文件中包含所有的文件
* 数据源：爬虫、数据库、HDFS


## 分词和分词器
* Lucece图形化工具Luke：https://github.com/DmitryKey/luke
* luke 
* 不同的field  有的需要构建索引，有的需要进行分词



### 分词算法概述
词是表达语言的最小单位。分词对搜索引擎很重要，可以帮助搜索引擎自动识别语句含义，使搜索结果匹配度达到最高，因此分词质量的好坏也
就影响了搜索结果的精度。利用分词器，把短语或者句子切分成分词，才能保证搜索的质量和结果

英文分词原理
基本流程：输入文本、词汇分割、词汇过滤（去Stop Word）、词干提取（形态还原）、大写转换，输出结果。


中文分词原理
中文分词比较复杂，因为中文句子中没有像英文中天然的空格来分隔。所以，中文分词主要有3种方法：
● 基于词典匹配
● 基于词频统计
● 基于语义统计
基于词典统计
       该算法实际上就是把一个句子从左向右扫描一遍，遇见字典中有的词就标记出来，遇到复合词就找到最长的词匹配，遇到不认识的词就切分城单个词。按照匹配扫描方向不同，又分为：
● 正向最大匹配
● 逆向最大匹配
● 最少切分
       真正实用的分词系统，都会把字典作为基础手段，再结合该语言自身的其他特征来提高切分的质量和精度。
基于词频统计
       该算法的原理是：在中文文章中，相邻的字搭配出现的频率越高，就越有可能形成一个固定的词。根据两个字的统计信息，计算两个汉字的相邻共现概率，统计出来的结果体现汉字之间的紧密程度，当紧密程度高于某个阈值，便可认为该组合可能构成一个词。
       基于词频统计的分词方法只需要对语料中的汉字组合频度进行统计，不需要词典，因此又叫做无词典分词或者统计分词法。
基于语义理解
       基于语义理解的分词是模拟人脑对语言和句子的理解，达到识别句子的效果。

Trie树（字典树）
       Trie树，又称字典树，单词查找树或者前缀树，是一种用于快速检索的多叉树结构，如英文字母的字典树是一个26叉树，数字的字典树是一个10叉树。它是一种哈希树的变种。典型应用是用于统计，排序和保存大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：利用字符串的公共前缀来减少查询时间，最大限度地减少无谓的字符串比较，查询效率比哈希树高。
1. 根节点不包含字符，除根节点外每一个节点都只包含一个字符。
2. 从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串。
3. 每个节点的所有子节点包含的字符都不相同。


Trie树（字典树）
Trie树，又称字典树，单词查找树或者前缀树，是一种用于快速检索的多叉树结构，如英文字母的字典树是一个26叉树，数字的字典树是一个10叉树。它是一种哈希树的变种。典型应用是用于统计，排序和保存大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：利用字符串的公共前缀来减少查询时间，最大限度地减少无谓的字符串比较，查询效率比哈希树高。
1. 根节点不包含字符，除根节点外每一个节点都只包含一个字符。
2. 从根节点到某一节点，路径上经过的字符连接起来，为该节点对应的字符串。
3. 每个节点的所有子节点包含的字符都不相同。


二、 Lucene分词器

索引和查询都是以词为基本单词。在Lucene中分词主要依靠**`Analyzer`**类解析实现。Analyzer内部主要通过TokenStream来实现，Tokenizer和TokenFilter是Tokan的两个子类。Tokenizer处理单个字符组成的字符流，读取Reader对象中的数据，处理后转换城词元。TokenFilter完成文本过滤功能。
Lucene分词类
StopAnalyzer
停用词分词器。过滤词汇中的特定字符串和词汇，并完成大写转小写的功能。
StandardAnalyzer
       根据空格和符号来完成分词，还可以完成数字、字母、Email地址、IP地址及中文字符的分析处理，还可以支持过滤词表，用来替代StopAnalyzer的过滤功能。
WhitespaceAnalyzer
       使用空格作为间隔的分割分词器。处理时，以空格作为分隔符号。分词器不做词汇过滤，也不进行大小写字符转换，主要用来特定环境下的英文符号处理。不需要字典支持。
SimpleAnalyzer
       处理时，以非字母字符作为分隔符号。分词器不做词汇过滤，值进行分析和分割。输出的结果完成大小写转换，去标点符号。不支持中文。不需要字典支持。
CJKAnalyzer
内部调用CJKTokenizer分词器，对中文进行分词，同时使用StopFilter完成过滤。
KeywordAnalyzer
       把整个输入作为一个单独次元，方便特殊文本索引和搜索。主要针对ZipCode、Address等表示特定语义的信息。

中文分词器
SmartChineseAnalyzer
运用概率知识，对中英文混合的文本进行分词操作，先将文本进行分句，再分别对每句话进行分词。这个分词器是基于隐马尔科夫模型而设计的，并使用了大量的语料进行中文词频的统计，同时包含了来自 ICTCLAS1.0的统计数据作为词典。
IKAnalayzer
https://github.com/blueshen/ik-analyzer/tree/master



三、 分词算法知识扩展

NLP的底层任务由易到难大致可以分为词法分析、句法分析和语义分析。分词是词法分析（还包括词性标注和命名实体识别）中最基本的任务，可以说既简单又复杂。分词算法根据其核心思想主要分为两种：
● 基于词典的分词，先把句子按照字典切分成词，再寻找词的最佳组合方式
● 基于字的分词，即由字构词，先把句子分成一个个字，再将字组合成词，寻找最优的切分策略，同时也可以转化成序列标注问题。
归根结底，上述两种方法都可以归结为在图或者概率图上寻找最短路径的问题。
基于词典的分词
最大匹配分词算法
最大匹配分词寻找最优组合的方式是将匹配到的最长词组合在一起。主要的思路是先将词典构造成一棵Trie树，也称为字典树
最短路径分词算法
最短路径分词算法首先将一句话中的所有词匹配出来，构成词图（有向无环图DAG），之后寻找从起始点到终点的最短路径作为最佳组合方式。
最短路径算法
基于Dijkstra算法求解最短路径。该算法适用于所有带权有向图，求解源节点到其他所有节点的最短路径，并可以求得全局最优解。Dijkstra本质为贪心算法，
在每一步走到当前路径最短的节点，递推地更新原节点到其他节点的距离。
N-最短路径算法
N-最短路径分词是对Dijkstra算法的扩展，在每一步保存最短的N条路径，并记录这些路径上当前节点的前驱，在最后求得最优解时回溯得到最短路径。
该方法的准确率优于Dijkstra算法，但在时间和空间复杂度上都更大。
基于NGram分词算法
基于字的分词
与基于词典的分词不同的是，基于字的分词事先不对句子进行词的匹配，而是将分词看成序列标注问题，把一个字标记成B(Begin), I(Inside), 
O(Outside), E(End), S(Single)。因此也可以看成是每个字的分类问题，输入为每个字及其前后字所构成的特征，输出为分类标记。对于分类问题，可以用统计机器学习或神经网络的方法求解。
生成式模型算法
生成式模型主要有NGram模型、HMM隐马尔可夫模型、朴素贝叶斯分类等。在分词中应用比较多的是n-gram模型和HMM模型。
判别式模型算法
判别式模型主要有感知机、SVM支持向量机、CRF条件随机场、最大熵模型等。在分词中常用的有感知机模型和CRF模型。
神经网络算法
在NLP中，最常用的神经网络为循环神经网络（RNN，Recurrent Neural Network），它在处理变长输入和序列输入问题中有着巨大的优势。LSTM为RNN变种的一种，
在一定程度上解决了RNN在训练过程中梯度消失和梯度爆炸的问题。

如何完成搜索过程
第一步
用户输入查询语句。举个例子，用户输入语句：lucene AND learned NOT hadoop。
第二步
对查询语句进行词法分析，语法分析，及语言处理。
1. 词法分析主要用来识别单词和关键字。如上述例子中，经过词法分析，得到单词有lucene，learned，hadoop, 关键字有AND, NOT。如果在词法分析中发现不合法的关键字，则会出现错误。如lucene AMD learned，其中由于AND拼错，导致AMD作为一个普通的单词参与查询。
2. 语法分析主要是根据查询语句的语法规则来形成一棵语法树。如果发现查询语句不满足语法规则，则会报错。如lucene NOT AND learned，则会出错。如上述例子，lucene AND learned NOT hadoop形成的语法树如下：
第三步
搜索索引，得到符合语法树的文档。此步骤有分几小步：
1. 首先，在反向索引表中，分别找出包含lucene，learn，hadoop的文档链表。
2. 其次，对包含lucene，learn的链表进行合并操作，得到既包含lucene又包含learn的文档链表。
3. 然后，将此链表与hadoop的文档链表进行差操作，去除包含hadoop的文档，从而得到既包含lucene又包含learn而且不包含hadoop的文档链表。
4. 此文档链表就是我们要找的文档。
第四步
根据得到的文档和查询语句的相关性，对结果进行排序。
虽然在上一步，我们得到了想要的文档，然而对于查询结果应该按照与查询语句的相关性进行排序，越相关者越靠前。如何计算文档和查询语句的相关性呢？我们可以把查询语句看作一片短小的文档，对文档与文档之间的相关性(relevance)进行打分(scoring)，分数高的相关性好，就应该排在前面。那么又怎么对文档之间的关系进行打分呢？ 
判断文档之间的关系，首先找出哪些词(Term)对文档之间的关系最重要，如search, Lucene, fulltext。然后判断这些词(Term)之间的关系。

下面仔细分析一下这两个过程：
	1. 计算权重(Term weight)的过程。影响一个词(Term)在一篇文档中的重要性主要有两个因素：
	○ Term Frequency (tf)：即此Term在此文档中出现了多少次。tf 越大说明越重要。
	○ Document Frequency (df)：即有多少文档包含次Term。df 越大说明越不重要。

	2. 判断Term之间的关系从而得到文档相关性的过程，也即向量空间模型的算法(VSM)。
我们把文档看作一系列词(Term)，每一个词(Term)都有一个权重(Term weight)，不同的词(Term)根据自己在文档中的权重来影响文档相关性的打分计算。
于是我们把所有此文档中词(term)的权重(term weight) 看作一个向量。

同样我们把查询语句看作一个简单的文档，也用向量来表示。

我们把所有搜索出的文档向量及查询向量放到一个N维空间中，每个词(term)是一维。


* TextField默认分词
* StringField 默认分词


## Lucene 查询高级应用

### 高亮显示
```xml
<dependency>
  <groupId>org.apache.lucene</groupId>
  <artifactId>lucene-highlighter</artifactId>
  <version>${lucene.version}</version>
</dependency>

```

### 分页查询
* 方式一 将所有的查询取回来，然后根据需要取当前页的数据。
* 方式二 通过某个固定的列分页（比如Id），这种查询方式只能对不变化的数据， 不能对其他的条件进行分页。
* 方式三 使用Lucene已经提供的IndexSearch.searchAfter()函数完成深度分页查询。

































